{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22529951de96682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06597e0c25dfdb3",
   "metadata": {},
   "source": [
    "# **Always run all imported notebooks when you make a change in some files you imported.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ebbd5e95ee751",
   "metadata": {},
   "source": [
    "### Adam Candrák/Mária Matušisková - 50%/50%\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c535da9f6d5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import connections\n",
    "# import devices\n",
    "# import normalize\n",
    "import processes\n",
    "# import profiles\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec93349090f70c",
   "metadata": {},
   "source": [
    "Just for test purposes to check if the import of jupyter notebooks was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d2bfab95821c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.test()\n",
    "# devices.test()\n",
    "processes.test()\n",
    "# profiles.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837ab784010a81a",
   "metadata": {},
   "source": [
    "# Phase 1 - Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7191672f40acf",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055f69efae0fad8",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc056f8b182350d",
   "metadata": {},
   "source": [
    "Change the names of columns of connections dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2159a344ec3135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_connections = connections.connections.rename(columns={\n",
    "    \"c.katana\": \"facebook\",\n",
    "    \"c.android.chrome\": \"chrome\",\n",
    "    \"c.android.gm\": \"gmail\",\n",
    "    \"c.dogalize\": \"dogalize\",\n",
    "    \"c.android.youtube\": \"youtube\",\n",
    "    \"c.updateassist\": \"updateassist\",\n",
    "    \"c.UCMobile.intl\": \"UCMobile.intl\",\n",
    "    \"c.raider\": \"raider\",\n",
    "    \"c.android.vending\": \"vending\",\n",
    "    \"c.UCMobile.x86\": \"UCMobile.x86\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7725c3136f348",
   "metadata": {},
   "source": [
    "Change the names of columns of processes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b980dd786bdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_processes = processes.processes.rename(columns={\n",
    "    \"p.katana\": \"facebook\",\n",
    "    \"p.android.chrome\": \"chrome\",\n",
    "    \"p.android.gm\": \"gmail\",\n",
    "    \"p.dogalize\": \"dogalize\",\n",
    "    \"p.android.vending\": \"vending\",\n",
    "    \"p.android.packageinstaller\": \"packageinstaller\",\n",
    "    \"p.system\": \"system\",\n",
    "    \"p.android.documentsui\": \"documentsui\",\n",
    "    \"p.android.settings\": \"settings\",\n",
    "    \"p.android.externalstorage\": \"externalstorage\",\n",
    "    \"p.android.defcontainer\": \"defcontainer\",\n",
    "    \"p.inputmethod.latin\": \"inputmethod.latin\",\n",
    "    \"p.process.gapps\": \"gapps\",\n",
    "    \"p.simulator\": \"simulator\",\n",
    "    \"p.android.gms\": \"google mobile services (gms)\",\n",
    "    \"p.google\": \"google\",\n",
    "    \"p.olauncher\": \"olauncher\",\n",
    "    \"p.browser.provider\": \"browser provider\",\n",
    "    \"p.notifier\": \"notifier\",\n",
    "    \"p.gms.persistent\": \"gms.persistent\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03220b683fbed7",
   "metadata": {},
   "source": [
    "Change type of timestamp to int64 of connections dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931119bb4a9b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_connections['ts'] = pd.to_datetime(c_connections['ts']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853132b3e9838afe",
   "metadata": {},
   "source": [
    "Change type of timestamp to int64 of processes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e602ec10b3fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_processes['ts'] = pd.to_datetime(p_processes['ts']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d49ba88dfd9a38",
   "metadata": {},
   "source": [
    "#### Merge datasets connections and processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2220abc70ae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.merge(c_connections, p_processes, on=['imei', 'ts', 'mwra'])\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7d533cffa799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387d38b080ea6e2",
   "metadata": {},
   "source": [
    "#### Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa9b8efc408d09",
   "metadata": {},
   "source": [
    "Find negative values in the merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ffaf162d3237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values = new_dataset.select_dtypes(include=[np.number]) < 0\n",
    "\n",
    "# any for columns and all values in the series of the first any\n",
    "has_negatives = negative_values.any().any()\n",
    "\n",
    "if has_negatives:\n",
    "    print(\"The dataset has negative values.\")\n",
    "    print(negative_values.any())  \n",
    "else:\n",
    "    print(\"No negative values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adee1e448f79c2b",
   "metadata": {},
   "source": [
    "Find NaN values in the merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fdd6c57bb5542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nan = new_dataset.isnull().values.any()\n",
    "\n",
    "if has_nan:\n",
    "    print(\"The dataset has NaN values.\")\n",
    "    print(new_dataset.isnull().values)\n",
    "else:\n",
    "    print(\"No NaN values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9456aacc1c2b9ee",
   "metadata": {},
   "source": [
    "Find duplicity values in the merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e1f915f6bbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_duplicity = new_dataset.duplicated().any()\n",
    "\n",
    "if has_duplicity:\n",
    "    print(\"The dataset has duplicity values.\")\n",
    "    print(new_dataset[new_dataset.duplicated()])\n",
    "    print(\"Number of duplicate rows:\", new_dataset.duplicated().sum())\n",
    "else:\n",
    "    print(\"No duplicity values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef1a9aebe8008e",
   "metadata": {},
   "source": [
    "Drop values which are not helpful for further training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed477876fafb215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.drop('ts', axis=1, inplace=True)\n",
    "new_dataset.drop('imei', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d25ae991406ada",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 2.1 B - Data integration\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Standard Deviation \n",
    "- detect outliers by standard deviation which spreads data around the mean\n",
    "- **3x standard deviations ($\\sigma$) from the mean ($\\mu$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2bf8721001598",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2f29a9f8c39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/code/marcinrutecki/outlier-detection-methods\n",
    "\n",
    "def StandardDevDetection(data, n, columns):\n",
    "    \n",
    "    outliers_inx = []\n",
    "    lower = 0\n",
    "    upper = 0\n",
    "\n",
    "    for column in columns:\n",
    "        # Calculate mean and standard derivation of each column\n",
    "        data_mean, data_std = mean(data[column], axis=0), std(data[column], axis=0)\n",
    "        print('column=', column, 'len=', len(data), 'mean=', data_mean, 'std=', data_std)\n",
    "\n",
    "        # Divide it to the three outliers in the standard deviations:\n",
    "        cut_off = data_std * 3\n",
    "        lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "        print('column=', column, 'cutoff=', cut_off, 'lower=', lower, 'upper=', upper)\n",
    "\n",
    "        # Filter the dataframe:\n",
    "        outliers = data[(data[column] < lower) | (data[column] > upper)].index\n",
    "        print('Identified outliers:', len(outliers))\n",
    "        \n",
    "        outliers_inx.extend(outliers)\n",
    "\n",
    "    outliers_inx = Counter(outliers_inx)\n",
    "    multiple_outliers = list( k for k, v in outliers_inx.items() if v > n )\n",
    "\n",
    "    data_uppper = data[data[column] > upper]\n",
    "    data_lower = data[data[column] < lower]\n",
    "    print('Total number of outliers is:', data_uppper.shape[0] + data_lower.shape[0])\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "\n",
    "columns = new_dataset.columns\n",
    "result = StandardDevDetection(new_dataset, 1, columns)\n",
    "\n",
    "new_dataset = new_dataset.drop(result, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c0f5ed8e5ba89",
   "metadata": {},
   "source": [
    "Divide it to the three outliers in the standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122dc74b84c9f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a16493b39aedc",
   "metadata": {},
   "source": [
    "Show data distribution after cut of outlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371263acc2d144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = new_dataset.columns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(len(columns)//2) - 4, ncols=3, figsize=(30, 30))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    data_mean, data_std = mean(new_dataset[col], axis=0), std(new_dataset[col], axis=0)\n",
    "    cut_off = data_std * 3\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    sns.histplot(new_dataset[col], kde=True, ax=axes[i])\n",
    "    axes[i].axvspan(xmin = lower,xmax= new_dataset[col].min(),alpha=0.2, color='red')\n",
    "    axes[i].axvspan(xmin = upper,xmax= new_dataset[col].max(),alpha=0.2, color='red')\n",
    "    axes[i].set_title(f'Distribution plot of the column {col}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c14547a7c676d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "## 2.1 A - Split dataset on Train and Test data\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81358451ac90e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c335fd6bcdc3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'mwra'\n",
    "mwra = new_dataset[target_column]\n",
    "data = new_dataset.drop(columns=[target_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fa12d3868f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_mwra, test_mwra = train_test_split(data, mwra, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cae62d8423699",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 2.1 B Data transformation\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dad87b5df8f4c",
   "metadata": {},
   "source": [
    "##### One-hot Encoding Binary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87819dd8f6849df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: IAU week-05\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# create object of BinaryEncoder\n",
    "ce_binary = ce.BinaryEncoder(cols = ['mwra'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_binary.fit_transform(train_mwra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c190cb31241bf49",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 2.1 C - Scaling\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756ebb0692c6b84",
   "metadata": {},
   "source": [
    "#### 2.1 C - Data normalization\n",
    "\n",
    "## $x_{normalization}=\\frac{x-x_{min}}{x_{max} - x_{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9cfc8e9655521",
   "metadata": {},
   "source": [
    "It is good to use, when in the data is a wide range of values. Which can lead to poor performance of models. Which is our case is true. We compared the data and the range was wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f13505ebc2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: IAU week-05\n",
    "\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# transform data\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fd35569710006",
   "metadata": {},
   "source": [
    "The result is in the numpy python format, which is the best format for training because is fast and intuitive compare to normal array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878504025fd5143",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 C - Data standardization\n",
    "\n",
    "## $x_{standardized} = \\frac{x -\\mu}{\\sigma}$\n",
    "where \n",
    "- $\\mu$ is the mean  of $x$\n",
    "- $\\sigma$ is the standard deviation of $x$\n",
    "\n",
    "It is a z-normalization, it measures variations of values about its mean in the dataset. (in short it measures the range of values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb08fafc6cd8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: IAU week-05\n",
    "\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8244471f38be5c",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 2.1 C - Make data distribution more Gaussian :3\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Power transformer 🤖 on random data -> Yeo-Johnson Transform with Linear Regression\n",
    "- to have more relatively similar to normally distributed\n",
    "\n",
    "This text is from the week-05:\n",
    "- Replacing the data with the log, square root, or inverse to remove skew\n",
    "- Yeo-Johnson transform (default): works with positive and negative values\n",
    "- Box-Cox transform: only works with strictly positive values\n",
    "- λ = −1.0 is a reciprocal transform.\n",
    "- λ = −0.5 is a reciprocal square root transform.  \n",
    "- λ = 0.0 is a log transform.\n",
    "- λ = 0.5 is a square root transform.\n",
    "- λ = 1.0 is no transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecec987136ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/code/abhikuks/power-transformers-in-depth-understanding\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from matplotlib import pyplot\n",
    "\n",
    "plt.figure(figsize=(30, 6))\n",
    "\n",
    "pt = PowerTransformer()\n",
    "train_data = pt.fit_transform(train_data)\n",
    "pyplot.hist(train_data, bins=10)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_data, train_mwra)\n",
    "\n",
    "# let's make some basic prediction on mwra\n",
    "predictions = lr.predict(train_data)\n",
    "print(predictions)\n",
    "\n",
    "# show trained transformed data in the histogram graph of normalized data\n",
    "plt.hist(train_data, bins=10, alpha=0.7, label='Transformed Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Transformed Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Transformed Training Data')\n",
    "pyplot.hist(train_data, bins=10)\n",
    "\n",
    "# show prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(train_mwra, predictions, alpha=0.6)\n",
    "plt.plot([train_mwra.min(), train_mwra.max()], [train_mwra.min(), train_mwra.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88058590d9bbfb41",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "#### 2.1 C - Data discretization \n",
    "-------------------------------------------------------------------------------------------\n",
    "- to reduce the effects of minor observation errors\n",
    "- minimalize the influence of outliers\n",
    "\n",
    "##### Equal-width dicretization\n",
    "\n",
    "- We chose this discretization as an example, because the data is still wide range between values and thus this could be a solution to cut range into the smaller intervals. It is called equally sized intervals. We might not use this method in further phases tho- we are unsure of how well the model will behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7317f6d72b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 8\n",
    "\n",
    "columns = columns.difference(['mwra'])\n",
    "train_data = pd.DataFrame(train_data, columns=columns)\n",
    "train_data_binned = pd.DataFrame()\n",
    "\n",
    "for column in train_data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    train_data_binned[f'{column}_binned'] = pd.cut(train_data[column], bins=num_bins, labels=False)\n",
    "\n",
    "print(train_data_binned.head())\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(len(columns)//2) - 5, ncols=3,figsize=(30, 50))\n",
    "axes = axes.flatten()\n",
    "i = 0\n",
    "for col in train_data_binned:\n",
    "    sns.histplot(train_data_binned[col], kde=True, ax=axes[i], color='blue', edgecolor='black')\n",
    "    axes[i].set_title(f'Equal-Width Binned Data for {col}')\n",
    "    i+= 1\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4a6487303061e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 2.1 D - Reasons for implementation\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870590b5b4d804f",
   "metadata": {},
   "source": [
    "Some things are already indicated in the text. However, we decided to merge processes and connections into one dataset since they have common columns and values. They were merged on ts, imei and mwra, these columns they had absolutely identical. \n",
    "In the dataset, there are not anymore ts and imei columns, because they were not adequate for testing purposes and we considered them as unnecessary. \n",
    "\n",
    "Furthermore, we renamed the columns to better names for reading and better orientation during work. For example, the katana represents Facebook. Why not change it to right away? \n",
    "\n",
    "After merging for sure, we checked if the data are clean enough for further work.  \n",
    "\n",
    "In our opinion, it is better to cut outliers before splitting the dataset. We also checked the presentation, and it was recommended to do it this way. After that, we split the dataset to train and test into two groups one is for all data and the second is for predicted value mwra. This way it is possible to predict how the mwra will behave. \n",
    "\n",
    "In conclusion, we prepared data for machine learning by data cleaning (missing values, outliers detection), integration (3x standard deviation, encoding) and transformation (scaling, transformers, discretization). Which ways we picked it are already defined in the specific sections.\n",
    "\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c538cd43de450b",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "## 2.2 Attribute selection\n",
    "--------------------------------\n",
    "\n",
    "### 2.2 A - Comparing feature selection methods\n",
    "\n",
    "### Filter Method\n",
    "\n",
    "As the first filter attribute selection method, we decided against using variance threshold method, as it did nothing. Instead we will use Mutual Information.\n",
    "\n",
    "**Mutual Information (MI)** is a measure of the mutual dependence between two variables. It quantifies how much information knowing one variable reduces the uncertainty about the other. In feature selection, mutual information tells us how informative a feature is about the target variable, regardless of whether the relationship is linear or non-linear.\n",
    "\n",
    "From Lab study material file- week-05:\n",
    "The concept of $MI$ is linked to information theory and **information entropy** ($\\mathcal{H}$). The unit of information depends on the base of the logarithm. If the base is 2, the most used, then the information is measured in *bits*. **MI is non-negative and symmetric.**\n",
    "\n",
    "#### $\\mathcal{H}(X) = - \\int dx~\\mu(x)~log\\mu(x)$\n",
    "#### $I(X, Y) = - \\int \\int dx~dy~\\mu(x, y)~log \\frac{\\mu(x,y)}{\\mu_x(x)~\\mu_y(y)}$\n",
    "\n",
    "For discrete variables,  $H(X)$ is calculated as:\n",
    "#### $H(X) = -\\sum_i P(x_{i}) log P(x_{i})$\n",
    "\n",
    "MI can be equivalently expressed as the amount of uncertainty in $X$ minus the amount of uncertainty in $X$ after $Y$ is known, denoted as\n",
    "\n",
    "#### $I(X; Y)= H(X)- H(X|Y)$\n",
    "\n",
    "The entropy of $X$ after observing values of $Y$ is formulated by\n",
    "#### $H(X|Y) = -\\sum_{\\substack{j}} P(y_{j}) \\sum_{\\substack{i}}P(x_{i}|y_{j})\\log_2{P(x_{i}|y_{j})}$\n",
    "\n",
    "where \n",
    "- $P(x_i)$ is the prior probabilities for all values of $X$ and \n",
    "- $P(x_i|y_i)$ is the posterior probabilities of $X$ given the values of $Y$. \n",
    "\n",
    "<sub><sup>(Adam note: Ngl plne nechapem tymto matematickym hieroglyfom)</sup></sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61568a31ab56319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "num_of_features = 10\n",
    "\n",
    "selector = mutual_info_classif(train_data, train_mwra)\n",
    "scores = pd.Series(selector, index=train_data.columns).sort_values(ascending=False)\n",
    "mi_selected_features = scores[:num_of_features].index\n",
    "\n",
    "print(\"The top \" + num_of_features.__str__() + \" features according to F-value method are: \")\n",
    "print(mi_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d9d2ba0abb114",
   "metadata": {},
   "source": [
    "\n",
    "As the second filter method, we decided to use **F-value** method since our data is continuous.\n",
    "\n",
    "**F-value** source: Lab study material file- week-05\n",
    "\n",
    "The correlation between each regressor and the target is computed by\n",
    "\n",
    "### $F_{value} = \\frac{variance_{dataset_1}}{variance_{dataset_2}} = \\frac{\\sigma_1^2}{\\sigma_2^2}$\n",
    "\n",
    "It is converted to an F score then to a p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15494a6129f9daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "num_of_features = 10\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=num_of_features)\n",
    "selector.fit(train_data, train_mwra)\n",
    "\n",
    "# get scores and p-values\n",
    "f_scores = pd.Series(selector.scores_, index=train_data.columns)\n",
    "f_scores_sorted = f_scores.sort_values(ascending=False)\n",
    "f_selected_features = f_scores_sorted.index[:num_of_features]\n",
    "\n",
    "print(f\"\\nThe top {num_of_features} features according to F-value method are:\")\n",
    "print(f_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b292791d427819b",
   "metadata": {},
   "source": [
    "### Wrapper\n",
    "\n",
    "These methods evaluate subsets of features by actually training a model, making them more accurate for specific algorithms.\n",
    "\n",
    "For showcasing wrapper method we chose RFE (Recursive feature elimination)\n",
    "\n",
    "RFE is a wrapper method that uses the built-in importance of the estimator attributes (in this case **RandomForest**) to generate the ratings. RFE is able to account for interactions between attributes, but is computationally more demanding than filter methods (note: look at the execution time of this cell lol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ff438836df58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "num_of_features = 10\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "#rfe\n",
    "rfe = RFE(estimator=model, n_features_to_select=num_of_features)\n",
    "tmp = rfe.fit_transform(train_data, train_mwra)\n",
    "\n",
    "rfe_selected_features = rfe.get_support(indices=True)\n",
    "\n",
    "rfe_selected_features = train_data.columns[rfe_selected_features]\n",
    "\n",
    "final_rf = RandomForestClassifier()\n",
    "final_rf.fit(train_data[rfe_selected_features], train_mwra)\n",
    "\n",
    "# get and sort features by importance\n",
    "rf_importance = pd.Series(final_rf.feature_importances_, index=rfe_selected_features)\n",
    "rfe_selected_features = rf_importance.sort_values(ascending=False).index\n",
    "\n",
    "print(f\"\\nThe top {num_of_features} features according to RFE method are:\")\n",
    "print(rfe_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5757ad8a1f7abc",
   "metadata": {},
   "source": [
    "So to recap everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13280d84c62b8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 10\n",
    "\n",
    "print(\"Columns selected by every method: \")\n",
    "mi_set = set(mi_selected_features)\n",
    "f_set = set(f_selected_features)\n",
    "rfe_set = set(rfe_selected_features)\n",
    "\n",
    "common_features = mi_set.intersection(f_set, rfe_set)\n",
    "print(\"\\nFeatures selected by all three methods:\")\n",
    "for feature in common_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': list(set(list(mi_selected_features) + list(f_selected_features) + list(rfe_selected_features))),\n",
    "    'Mutual Information': False,\n",
    "    'F-Value': False,\n",
    "    'RFE': False\n",
    "})\n",
    "\n",
    "comparison_df.loc[comparison_df['Feature'].isin(mi_selected_features), 'Mutual Information'] = True\n",
    "comparison_df.loc[comparison_df['Feature'].isin(f_selected_features), 'F-Value'] = True\n",
    "comparison_df.loc[comparison_df['Feature'].isin(rfe_selected_features), 'RFE'] = True\n",
    "\n",
    "################\n",
    "# help with stylization by Claude.ai\n",
    "################\n",
    "\n",
    "def color_boolean(val):\n",
    "    if isinstance(val, bool):\n",
    "        color = '#90EE90' if val else '#FFB6C1'  # Light green if True, light red if False\n",
    "        return f'background-color: {color}'\n",
    "    return ''\n",
    "\n",
    "# Apply styling to the DataFrame\n",
    "styled_df = (comparison_df.style\n",
    "             .map(color_boolean, subset=['Mutual Information', 'F-Value', 'RFE'])\n",
    "             .set_properties(**{\n",
    "    'text-align': 'center',\n",
    "    'border': '1px solid gray',\n",
    "    'padding': '8px'\n",
    "})\n",
    "             .set_properties(subset=['Feature'], **{\n",
    "    'text-align': 'left',\n",
    "    'font-weight': 'bold',\n",
    "    'background-color': '#F0F8FF'  # Light blue background for feature names\n",
    "})\n",
    "             .set_table_styles([\n",
    "    {'selector': 'th',\n",
    "     'props': [('background-color', '#4682B4'),  # Steel blue headers\n",
    "               ('color', 'white'),\n",
    "               ('font-weight', 'bold'),\n",
    "               ('text-align', 'center'),\n",
    "               ('padding', '8px'),\n",
    "               ('border', '1px solid gray')]},\n",
    "    {'selector': 'caption',\n",
    "     'props': [('caption-side', 'top'),\n",
    "               ('font-size', '16px'),\n",
    "               ('font-weight', 'bold'),\n",
    "               ('color', '#2F4F4F'),  # Dark slate gray\n",
    "               ('padding', '8px')]}\n",
    "])\n",
    "             .set_caption('Feature Selection Methods Comparison'))\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_df)\n",
    "\n",
    "# sets of features unique to each method\n",
    "mi_unique = mi_set - (f_set | rfe_set)\n",
    "f_unique = f_set - (mi_set | rfe_set)\n",
    "rfe_unique = rfe_set - (mi_set | f_set)\n",
    "\n",
    "print(\"\\nFeatures unique to each method:\")\n",
    "print(\"Mutual Information only:\", mi_unique if mi_unique else \"None\")\n",
    "print(\"F-Value only:\", f_unique if f_unique else \"None\")\n",
    "print(\"RFE only:\", rfe_unique if rfe_unique else \"None\")\n",
    "\n",
    "################\n",
    "# source: Claude.ia\n",
    "################\n",
    "\n",
    "def create_ranking_comparison(mi_features, f_features, rfe_features):\n",
    "    # Create a dictionary to store rankings\n",
    "    rankings = {}\n",
    "\n",
    "    # Add rankings for each method (1 being highest importance)\n",
    "    for i, feature in enumerate(mi_features):\n",
    "        if feature not in rankings:\n",
    "            rankings[feature] = {'MI_rank': i + 1}\n",
    "        else:\n",
    "            rankings[feature]['MI_rank'] = i + 1\n",
    "\n",
    "    for i, feature in enumerate(f_features):\n",
    "        if feature not in rankings:\n",
    "            rankings[feature] = {'F_rank': i + 1}\n",
    "        else:\n",
    "            rankings[feature]['F_rank'] = i + 1\n",
    "\n",
    "    for i, feature in enumerate(rfe_features):\n",
    "        if feature not in rankings:\n",
    "            rankings[feature] = {'RFE_rank': i + 1}\n",
    "        else:\n",
    "            rankings[feature]['RFE_rank'] = i + 1\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    ranking_df = pd.DataFrame.from_dict(rankings, orient='index')\n",
    "\n",
    "    # Fill NaN with 0 (features not selected by a method)\n",
    "    ranking_df = ranking_df.fillna(0)\n",
    "\n",
    "    # Calculate average rank (excluding zeros)\n",
    "    ranking_df['Avg_Rank'] = ranking_df.replace(0, np.nan).mean(axis=1)\n",
    "\n",
    "    # Count methods that selected each feature\n",
    "    ranking_df['Methods_Count'] = (ranking_df[['MI_rank', 'F_rank', 'RFE_rank']] != 0).sum(axis=1)\n",
    "\n",
    "    # Sort by Methods_Count (descending) and then by Avg_Rank (ascending)\n",
    "    ranking_df = ranking_df.sort_values(['Methods_Count', 'Avg_Rank'], ascending=[False, True])\n",
    "\n",
    "    return ranking_df\n",
    "\n",
    "# Create the ranking comparison\n",
    "ranking_comparison = create_ranking_comparison(\n",
    "    mi_selected_features,\n",
    "    f_selected_features,\n",
    "    rfe_selected_features\n",
    ")\n",
    "\n",
    "ranking_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30446bb3c00e820",
   "metadata": {},
   "source": [
    "From our observations, we can see that Mutual Information and F-value selected the same features. There is also a big overlap with RFE that selected eight of the features that the other two methods selected.  While MI and F-value selected facebook.y and inputmethod.latin RFE selected UCMobile.x86 and google- clearly showing that these features have more complex relationships with mwra.\n",
    "\n",
    "Furthermore, the one feature consistently considered as the most important is gmail_x followed by gapps, facebook_x and chrome_x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c44346b817be34",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "## Phase 3\n",
    "-------------------------------------------------------------------------------------------\n",
    "### 3.1 Simple classification based on dependencies in data\n",
    "\n",
    "First, let's import our implementation of ID3 and data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca02205faa9437d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:49:36.602119Z",
     "start_time": "2024-11-11T19:49:35.348144Z"
    }
   },
   "source": [
    "import import_ipynb\n",
    "import id3_implementation as id3\n",
    "import preprocessing as pp"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values found in the dataset.\n",
      "The dataset has duplicity values.\n",
      "                        ts                 imei  mwra  facebook_x  chrome_x  \\\n",
      "95     1525520040000000000  8630330696303482303   1.0    10.82916  11.29582   \n",
      "108    1525520760000000000   863033069630348065   1.0    14.71992  14.01692   \n",
      "175    1525524720000000000   359043379931766353   0.0     8.46728  13.41079   \n",
      "300    1525532160000000000  8630330696303481669   1.0     9.87679   8.52849   \n",
      "303    1525532280000000000   359043379931766924   1.0    10.22849  11.46160   \n",
      "...                    ...                  ...   ...         ...       ...   \n",
      "15445  1525987200000000000  3590433799317661925   1.0    11.23638  12.54494   \n",
      "15446  1526140320000000000   863033069630348776   0.0    11.71795  13.86245   \n",
      "15447  1526140320000000000   863033069630348776   0.0    11.71795  13.86245   \n",
      "15448  1526338140000000000  3590433799317661206   1.0    15.51197  16.53309   \n",
      "15449  1526338140000000000  3590433799317661206   1.0    15.51197  16.53309   \n",
      "\n",
      "        gmail_x  dogalize_x   youtube  updateassist  UCMobile.intl  ...  \\\n",
      "95     12.66003    10.16379  16.04734      39.11921       41.94816  ...   \n",
      "108    14.46679    10.37614  15.28242      54.88030       54.02226  ...   \n",
      "175    11.08708     6.18605   9.60942      52.78819       56.42743  ...   \n",
      "300     9.57042    11.81096   7.54489       7.94035       61.92247  ...   \n",
      "303     8.69223    10.45511  12.44206      46.64713       52.17634  ...   \n",
      "...         ...         ...       ...           ...            ...  ...   \n",
      "15445  11.26646     9.03636  12.76080      40.59272       30.60551  ...   \n",
      "15446  12.07446    10.10313  13.96660      45.07102       52.21975  ...   \n",
      "15447  12.07446    10.10313  13.96660      45.07102       52.21975  ...   \n",
      "15448  15.75924    11.99555  14.99913      43.95935       35.75699  ...   \n",
      "15449  15.75924    11.99555  14.99913      43.95935       35.75699  ...   \n",
      "\n",
      "       dogalize_y     gapps  simulator  facebook_y  \\\n",
      "95       97.52469  95.01733   56.82875    81.39681   \n",
      "108      87.62723  75.66986   13.90231    10.51647   \n",
      "175      30.91028   1.84816   36.52292     8.24216   \n",
      "300      75.93998  80.95502   86.96990     1.35263   \n",
      "303      61.25695  57.35034   36.31488    67.14928   \n",
      "...           ...       ...        ...         ...   \n",
      "15445    87.48802   1.93764   55.19853    61.24749   \n",
      "15446    54.55465  34.82606   68.63334    98.60112   \n",
      "15447    54.55465  34.82606   68.63334    98.60112   \n",
      "15448    74.14262  50.12292   55.73990    82.29958   \n",
      "15449    74.14262  50.12292   55.73990    82.29958   \n",
      "\n",
      "       google mobile services (gms)    google  olauncher  browser provider  \\\n",
      "95                         15.49079  33.46497   47.35665           9.96695   \n",
      "108                        36.08753  32.65558   13.72486          74.68378   \n",
      "175                        85.18883  85.80315   28.04099           5.37063   \n",
      "300                        33.50430   7.26696   23.43879          90.34312   \n",
      "303                        38.96647  56.28646   88.96592          11.03558   \n",
      "...                             ...       ...        ...               ...   \n",
      "15445                      27.68095   7.42961   73.85143          98.09352   \n",
      "15446                      20.39117  17.48535   97.03999          59.31267   \n",
      "15447                      20.39117  17.48535   97.03999          59.31267   \n",
      "15448                      34.64708  85.38809   68.57332          50.53580   \n",
      "15449                      34.64708  85.38809   68.57332          50.53580   \n",
      "\n",
      "       notifier  gms.persistent  \n",
      "95      9.45719        90.77472  \n",
      "108    47.71853        10.51180  \n",
      "175    64.96796        16.24571  \n",
      "300    43.77779        19.04622  \n",
      "303    36.52814        89.92218  \n",
      "...         ...             ...  \n",
      "15445  39.51722        29.41750  \n",
      "15446  97.53013         7.77545  \n",
      "15447  97.53013         7.77545  \n",
      "15448  61.50573        81.83581  \n",
      "15449  61.50573        81.83581  \n",
      "\n",
      "[537 rows x 33 columns]\n",
      "Number of duplicate rows: 537\n",
      "Processed training data shape: (10402, 9)\n",
      "Processed test data shape: (4458, 9)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "8bbc98a33c0c7075",
   "metadata": {},
   "source": [
    "Now, let's copy the preprocessed data from the notebook and build the tree.\n",
    "\n",
    "**BIG NOTE**: This little maneuver is gonna cost us 51 years (or roughly 7 minutes)\n",
    "\n",
    "<sub><sup>(Shout out to everyone who got the Interstellar ref.)</sup></sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6007d6da349aae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:49:38.310971Z",
     "start_time": "2024-11-11T19:49:38.306801Z"
    }
   },
   "source": [
    "# Copy and convert to np types the preprocessed data for purposes of training the ID3 tree\n",
    "# (Adam Note: We don't need to copy the data, we can still just read the csv files we created- I was not sure which\n",
    "# one to do so...)\n",
    "\n",
    "y_train = pp.y_train_df.to_numpy()\n",
    "X_train =  pp.X_train_processed_df.to_numpy()\n",
    "y_test = pp.y_test_df.to_numpy()\n",
    "X_test = pp.X_test_processed_df.to_numpy()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "41593394046354b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:04:02.444256Z",
     "start_time": "2024-11-11T19:57:02.686516Z"
    }
   },
   "source": [
    "# Train the ID3 tree (without a depth limit)\n",
    "id3_tree = id3.build_tree(X_train, y_train.flatten(), -1) # flatten - bc the function expects 1D array for y"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "aae4dab67614e03f",
   "metadata": {},
   "source": [
    "### 3.1 B Evaluation of the model\n",
    "\n",
    "Now, let's evaluate the model on the test data with metrics like accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f9c2d41a3585f4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:05:28.772165Z",
     "start_time": "2024-11-11T20:05:28.747332Z"
    }
   },
   "source": [
    "# make predictions\n",
    "y_prediction = []\n",
    "for i in range(len(X_test)):\n",
    "    y_prediction.append(id3.predict(id3_tree, X_test[i]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "id3_accuracy = accuracy_score(y_test, y_prediction)\n",
    "print(f\"Accuracy: {id3_accuracy:.4f}\")\n",
    "\n",
    "id3_precision = precision_score(y_test, y_prediction, average='weighted')\n",
    "print(f\"Precision: {id3_precision:.4f}\")\n",
    "\n",
    "id3_recall = recall_score(y_test, y_prediction, average='weighted')\n",
    "print(f\"Recall: {id3_recall:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777\n",
      "Precision: 0.7779\n",
      "Recall: 0.7777\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "cdbd2d3273056c3c",
   "metadata": {},
   "source": [
    "### 3.1 C Overfitting\n",
    "\n",
    "Now, let's look at overfitting. We will evaluate the model on the training data.\n",
    "\n",
    "Overfitting is commonly detected by comparing the accuracy of the model on the training data and the test data. If the model has a higher accuracy on the training data, it is likely overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33d91daf31a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for overfitting\n",
    "# create predictions for the training data\n",
    "y_train_prediction = []\n",
    "for i in range(len(X_train)):\n",
    "    y_train_prediction.append(id3.predict(id3_tree, X_train[i]))\n",
    "\n",
    "id3_train_accuracy = accuracy_score(y_train, y_train_prediction)\n",
    "print(f\"Training Accuracy: {id3_train_accuracy:.2f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "if id3_train_accuracy > id3_accuracy:\n",
    "    print(\"The model is overfitting.\")\n",
    "else:\n",
    "    print(\"The model is not overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483961a3479f1b06",
   "metadata": {},
   "source": [
    "Here the accuracy with training data for this model is 1.0, which is a clear sign of overfitting. The model is too complex and it is not generalizing well to new data.\n",
    "\n",
    "To be honest, after writing this, we realized that we can improve the overfitting by limiting the depth of the tree. We will improve our implementation and re-run the test.\n",
    "\n",
    "Now we will try different possible limits with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562614bba8ee1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ID3 tree with a depth limit\n",
    "id3_tree_depth_limited = id3.build_tree(X_train, y_train.flatten(), 9)\n",
    "\n",
    "# make predictions\n",
    "y_prediction_depth_limited = []\n",
    "for i in range(len(X_test)):\n",
    "    y_prediction_depth_limited.append(id3.predict(id3_tree_depth_limited, X_test[i]))\n",
    "\n",
    "# evaluate the model\n",
    "id3_accuracy_depth_limited = accuracy_score(y_test, y_prediction_depth_limited)\n",
    "print(f\"Accuracy: {id3_accuracy_depth_limited:.4f}\")\n",
    "\n",
    "id3_precision_depth_limited = precision_score(y_test, y_prediction_depth_limited, average='weighted')\n",
    "print(f\"Precision: {id3_precision_depth_limited:.4f}\")\n",
    "\n",
    "id3_recall_depth_limited = recall_score(y_test, y_prediction_depth_limited, average='weighted')\n",
    "print(f\"Recall: {id3_recall_depth_limited:.4f}\")\n",
    "\n",
    "# create predictions for the training data\n",
    "y_train_prediction_depth_limited = []\n",
    "for i in range(len(X_train)):\n",
    "    y_train_prediction_depth_limited.append(id3.predict(id3_tree_depth_limited, X_train[i]))\n",
    "\n",
    "id3_train_accuracy_depth_limited = accuracy_score(y_train, y_train_prediction_depth_limited)\n",
    "print(f\"Training Accuracy: {id3_train_accuracy_depth_limited:.2f}\")\n",
    "\n",
    "# Check for overfitting (found this was not correct...)\n",
    "# if id3_train_accuracy_depth_limited > id3_accuracy_depth_limited:\n",
    "#     print(\"The model is overfitting.\")\n",
    "# else:\n",
    "#     print(\"The model is not overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308086e644f5847",
   "metadata": {},
   "source": [
    "Results after limiting the depth of the tree to **5**:\n",
    "- Accuracy: 0.8120\n",
    "- Precision: 0.8127\n",
    "- Recall: 0.8120\n",
    "- Training Accuracy: 0.83\n",
    "\n",
    "The accuracy is much better than without the depth limit. Let's try to limit the depth even further.\n",
    "\n",
    "Results after limiting the depth of the tree to **4**:\n",
    "- Accuracy: 0.8053\n",
    "- Precision: 0.8071\n",
    "- Recall: 0.8053\n",
    "- Training Accuracy: 0.82\n",
    "\n",
    "Now we see that the accuracy is slightly worse, but the model might be underfitting (after further testing, it seems like the tree is underfitted). Now let's try to limit the depth to 6.\n",
    "\n",
    "Results after limiting the depth of the tree to **6**:\n",
    "- Accuracy: 0.8356\n",
    "- Precision: 0.8353\n",
    "- Recall: 0.8356\n",
    "- Training Accuracy: 0.86\n",
    "\n",
    "The accuracy is better than with the depth limit of 5. Let's try limit 7.\n",
    "\n",
    "Results after limiting the depth of the tree to **7**:\n",
    "- Accuracy: 0.8374\n",
    "- Precision: 0.8373\n",
    "- Recall: 0.8374\n",
    "- Training Accuracy: 0.87\n",
    "\n",
    "Results after limiting the depth of the tree to **9**:\n",
    "- Accuracy: 0.8407\n",
    "- Precision: 0.8407\n",
    "- Recall: 0.8407\n",
    "- Training Accuracy: 0.89\n",
    "\n",
    "The accuracy is even better!\n",
    "\n",
    "(Adam: I went on a bit of a journey with this...)\n",
    "\n",
    "At this point, we can just generate and plot the metrics for different depths, even though it's beyond the scope of the assignment.\n",
    "\n",
    "NOTE: We turned this cell into a \"raw\" cell to avoid running it by accident. (Run time was roughly 1,5 hours)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "746d6fb4-5ecc-45fe-8628-9c667bd6f506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T14:43:12.891369Z",
     "start_time": "2024-11-09T13:04:20.785557Z"
    }
   },
   "source": [
    "id3_accuracy_list = []\n",
    "id3_precision_list = []\n",
    "id3_recall_list = []\n",
    "id3_train_accuracy_list = []\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "depths = list(range(5, 20))\n",
    "\n",
    "for depth in depths:\n",
    "    id3_tree_depth_limited = id3.build_tree(X_train, y_train.flatten(), depth)\n",
    "\n",
    "    y_prediction_depth_limited = []\n",
    "    for i in range(len(X_test)):\n",
    "        y_prediction_depth_limited.append(id3.predict(id3_tree_depth_limited, X_test[i]))\n",
    "\n",
    "    id3_accuracy_depth_limited = accuracy_score(y_test, y_prediction_depth_limited)\n",
    "    id3_accuracy_list.append(id3_accuracy_depth_limited)\n",
    "\n",
    "    id3_precision_depth_limited = precision_score(y_test, y_prediction_depth_limited, average='weighted')\n",
    "    id3_precision_list.append(id3_precision_depth_limited)\n",
    "\n",
    "    id3_recall_depth_limited = recall_score(y_test, y_prediction_depth_limited, average='weighted')\n",
    "    id3_recall_list.append(id3_recall_depth_limited)\n",
    "\n",
    "    y_train_prediction_depth_limited = []\n",
    "    for i in range(len(X_train)):\n",
    "        y_train_prediction_depth_limited.append(id3.predict(id3_tree_depth_limited, X_train[i]))\n",
    "\n",
    "    id3_train_accuracy_depth_limited = accuracy_score(y_train, y_train_prediction_depth_limited)\n",
    "    id3_train_accuracy_list.append(id3_train_accuracy_depth_limited)\n",
    "\n",
    "\n",
    "# Plot accuracy, precision, and recall for test set\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(depths, id3_accuracy_list, marker='o', color='blue', label='Test Accuracy')\n",
    "plt.plot(depths, id3_train_accuracy_list, marker='o', color='orange', label='Train Accuracy')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Depth')\n",
    "plt.legend()\n",
    "\n",
    "# Precision Plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(depths, id3_precision_list, marker='o', color='green')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Depth')\n",
    "\n",
    "# Recall Plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(depths, id3_recall_list, marker='o', color='red')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall vs Depth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5edf413b1e56a",
   "metadata": {},
   "source": [
    "Results: ![ID3 - depth testing](images/ID3_depth_testing.png \"id3_depth_testing\")\n",
    "\n",
    "With this testing we can see that this model can get easily and extremely overfitted. The accuracy is the best with depth 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a218d3f42d29a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "### 3.2 Training and Evaluation of Machine Learning Classifiers\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "### 3.2 A - Tree algorithm from scikit-learn\n",
    "\n",
    "While with ID3 we show pre-pruning by limiting the depth of the three. We will now use post-pruning while showcasing the use of DecisionTreeClassifier from scikit-learn to train a tree model.\n",
    "\n",
    "### Pruning\n",
    "\n",
    "For post-pruning, we will use [Minimal Cost-Complexity Pruning](https://scikit-learn.org/1.5/modules/tree.html#minimal-cost-complexity-pruning) (MCCP). This method prunes the tree by removing nodes that have a cost complexity less than a threshold $\\alpha$.\n",
    "\n",
    "### DecisionTreeClassifier or RandomTreeRegressor\n",
    "Now, considering which three from scikit-learn to use — weather DecisionTreeClassifier or RandomTreeRegressor. Since our problem is a classification problem (predicting mwra), we decided to use DecisionTreeClassifier. \n",
    "\n",
    "### Gini Impurity\n",
    "For criteria, we chose gini, which is the default one. Gini impurity is an alternative metric to entropy that is also used to measure the quality of a split in decision tree classification algorithms. Like entropy, Gini impurity quantifies the level of impurity or uncertainty in a dataset, but it does so in a slightly different way.\n",
    "\n",
    "Gini impurity, denoted as G(X), is defined as the probability of a randomly chosen element being incorrectly classified if it was randomly labeled according to the distribution of labels in the dataset. Mathematically, it is calculated as:\n",
    "$$G(X) = 1 - Σ p(x)^2$$\n",
    "Where p(x) is the probability mass function of the random variable X (the class labels).\n",
    "\n",
    "Let's train the model and evaluate it without any pruning first."
   ]
  },
  {
   "cell_type": "code",
   "id": "39227caa7b2d27fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:07:10.819682Z",
     "start_time": "2024-11-11T20:07:10.661117Z"
    }
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Train the DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_prediction_dtc = dtc.predict(X_test)\n",
    "\n",
    "dtc_accuracy = accuracy_score(y_test, y_prediction_dtc)\n",
    "print(f\"Accuracy: {dtc_accuracy:.4f}\")\n",
    "\n",
    "dtc_precision = precision_score(y_test, y_prediction_dtc, average='weighted')\n",
    "print(f\"Precision: {dtc_precision:.4f}\")\n",
    "\n",
    "dtc_recall = recall_score(y_test, y_prediction_dtc, average='weighted')\n",
    "print(f\"Recall: {dtc_recall:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7741\n",
      "Precision: 0.7738\n",
      "Recall: 0.7741\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results are slightly worse than with ID3 which is surprising. Now, let's try to prune the tree using MCCP.",
   "id": "d25a866a27453568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:52:46.865181Z",
     "start_time": "2024-11-11T20:52:46.707386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the DecisionTreeClassifier with pruning\n",
    "dtc_pruned = DecisionTreeClassifier(criterion='gini', random_state=42, ccp_alpha=0.001)\n",
    "dtc_pruned.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_prediction_dtc = dtc_pruned.predict(X_test)\n",
    "\n",
    "dtc_accuracy = accuracy_score(y_test, y_prediction_dtc)\n",
    "print(f\"Accuracy: {dtc_accuracy:.4f}\")\n",
    "\n",
    "dtc_precision = precision_score(y_test, y_prediction_dtc, average='weighted')\n",
    "print(f\"Precision: {dtc_precision:.4f}\")\n",
    "\n",
    "dtc_recall = recall_score(y_test, y_prediction_dtc, average='weighted')\n",
    "print(f\"Recall: {dtc_recall:.4f}\")\n"
   ],
   "id": "c54c4375f3bc7766",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8385\n",
      "Precision: 0.8388\n",
      "Recall: 0.8385\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The results are slightly better than without pruning. \n",
    "\n",
    "### Finding the best $\\alpha$\n",
    "\n",
    "Now, let's try to find the best $\\alpha$ value for pruning. We will use cross-validation to find the best $\\alpha$ value. (We know it's too early and it is required further in the assignment, but we couldn't help it)"
   ],
   "id": "813a8223e0c5ce66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T21:32:32.380671Z",
     "start_time": "2024-11-11T21:24:45.609452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = dtc.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas[:-1]  # minus the last value as it is the root node\n",
    "\n",
    "dtcs = []\n",
    "\n",
    "# go through each possible alpha value from the path\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dtc = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    dtcs.append(dtc)\n",
    "\n",
    "# use of cross-validation\n",
    "train_scores = [cross_val_score(dtc, X_train, y_train, cv=5).mean() for dtc in dtcs]\n",
    "\n",
    "best_alpha_index = train_scores.index(max(train_scores))\n",
    "best_alpha = ccp_alphas[best_alpha_index]\n",
    "print(f\"Best alpha: {best_alpha:.4f} with Cross-validation Accuracy: {train_scores[best_alpha_index]:.4f}\")\n",
    "\n",
    "# Train final model with best alpha\n",
    "best_model = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
    "best_model.fit(X_train, y_train)"
   ],
   "id": "643ef4b6b973adbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.0007 with Cross-validation Accuracy: 0.8509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=np.float64(0.0006913108988314022),\n",
       "                       random_state=42)"
      ],
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(ccp_alpha=np.float64(0.0006913108988314022),\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(ccp_alpha=np.float64(0.0006913108988314022),\n",
       "                       random_state=42)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9bde5cd75789cb57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
